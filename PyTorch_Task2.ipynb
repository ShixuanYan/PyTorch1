{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于梯度下降，我的理解是：在每一轮迭代中，利用损失函数相对于模型的负梯度方向的信息来对当前模型进行更新，模型以参数化的形式表示，从而模型的更新等价于参数的更新。下一个点由当前点减去当前点的梯度二来，同时为了控制更新速度，在梯度前乘以一个系数，即learning rate。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1、numpy和pytorch实现梯度下降法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=100, out_features=300, bias=True)\n",
      "  (fc2): Linear(in_features=300, out_features=100, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "NUM = 100   #输入个数（输入层神经元个数）\n",
    "hider_num = 300  #隐藏层神经元个数\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    " \n",
    "        self.fc1 = nn.Linear(NUM,hider_num)\n",
    "        self.fc2 = nn.Linear(hider_num,NUM)\n",
    " \n",
    "    def forward(self,x):\n",
    "        x = f.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "x = torch.randn(NUM)\n",
    "input = Variable(x)   #随机生成NUM个数据\n",
    " \n",
    "target = Variable(0.5 * x + 0.3)   #用0.5 × x + 0.3 函生成目标数据\n",
    "\n",
    "net = Net()   #网络对象\n",
    "print(net)\n",
    " \n",
    "optimizer = optim.SGD(net.parameters(),lr=0.01)  #随机梯度下降优化器\n",
    "loss_list =[]                                #保存loss，便于画图\n",
    "step = 500                                    #迭代次数\n",
    " \n",
    "for epoch in range(step):\n",
    "    optimizer.zero_grad()                    #参数梯度清零，因为会累加\n",
    "    out = net(input)                         #通过一次网络的输出\n",
    "    loss = nn.MSELoss()(out,target)           #计算输出与target数据的均方差\n",
    "    loss_list.append(loss)                    #保存loss\n",
    "    loss.backward()                           #loss反向传播\n",
    "    optimizer.step()                          #更新参数w，b\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(range(1,NUM+1),target.detach().numpy().tolist(),'*',ms=10,lw=1,color='black')\n",
    "plt.plot(range(1,NUM+1),out.detach().numpy().tolist(),'o',ms=3,lw=1,color='red')\n",
    "plt.show()   #画出target和输出的位置图\n",
    "plt.figure(2)\n",
    "plt.plot(range(1,step+1),loss_list,'o-',ms=3,lw=1,color='black')\n",
    "plt.show()   #画loss图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2、设定初始值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(2, 2, requires_grad=True)\n",
    "learning_rate =0.1 #学习率\n",
    "epoches =10 #学习周期"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3&4、求取梯度 & 在梯度方向上进行参数的更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad tensor([[2.4586, 3.1636],\n",
      "        [2.7006, 2.5012]])\n",
      "grad tensor([[1.9669, 2.5309],\n",
      "        [2.1605, 2.0010]])\n",
      "grad tensor([[1.5735, 2.0247],\n",
      "        [1.7284, 1.6008]])\n",
      "grad tensor([[1.2588, 1.6198],\n",
      "        [1.3827, 1.2806]])\n",
      "grad tensor([[1.0070, 1.2958],\n",
      "        [1.1062, 1.0245]])\n",
      "tensor([[-0.5972, -0.4817],\n",
      "        [-0.5575, -0.5902]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.rand(2, 2, requires_grad=True)\n",
    "learning_rate =0.1 #学习率\n",
    "epoches =5 #学习周期\n",
    "\n",
    "for epoch in range(epoches):\n",
    "     y = x**2+2*x+1\n",
    "     y.backward(torch.ones_like(x))\n",
    "     print(\"grad\",x.grad.data) #x的梯度值\n",
    "     x.data = x.data - learning_rate*x.grad.data #更新x\n",
    "     x.grad.data.zero_()\n",
    "print(x.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5、numpy和pytorch实现线性回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch's loss: 8.379321098327637\n",
      "1 epoch's loss: 0.3123200833797455\n",
      "2 epoch's loss: 0.20587734878063202\n",
      "3 epoch's loss: 0.19500373303890228\n",
      "4 epoch's loss: 0.18572787940502167\n",
      "5 epoch's loss: 0.1769055277109146\n",
      "6 epoch's loss: 0.16850236058235168\n",
      "7 epoch's loss: 0.16049836575984955\n",
      "8 epoch's loss: 0.1528746783733368\n",
      "9 epoch's loss: 0.1456129252910614\n",
      "10 epoch's loss: 0.13869620859622955\n",
      "11 epoch's loss: 0.13210803270339966\n",
      "12 epoch's loss: 0.12583279609680176\n",
      "13 epoch's loss: 0.119855597615242\n",
      "14 epoch's loss: 0.11416245996952057\n",
      "15 epoch's loss: 0.10873966664075851\n",
      "16 epoch's loss: 0.10357439517974854\n",
      "17 epoch's loss: 0.09865453094244003\n",
      "18 epoch's loss: 0.09396837651729584\n",
      "19 epoch's loss: 0.08950488269329071\n",
      "20 epoch's loss: 0.0852532684803009\n",
      "21 epoch's loss: 0.08120360225439072\n",
      "22 epoch's loss: 0.07734647393226624\n",
      "23 epoch's loss: 0.07367238402366638\n",
      "24 epoch's loss: 0.07017296552658081\n",
      "25 epoch's loss: 0.06683965027332306\n",
      "26 epoch's loss: 0.06366468966007233\n",
      "27 epoch's loss: 0.06064063310623169\n",
      "28 epoch's loss: 0.05776016414165497\n",
      "29 epoch's loss: 0.05501648411154747\n",
      "30 epoch's loss: 0.05240318179130554\n",
      "31 epoch's loss: 0.04991397634148598\n",
      "32 epoch's loss: 0.04754302650690079\n",
      "33 epoch's loss: 0.045284777879714966\n",
      "34 epoch's loss: 0.043133676052093506\n",
      "35 epoch's loss: 0.041084788739681244\n",
      "36 epoch's loss: 0.03913319855928421\n",
      "37 epoch's loss: 0.03727438673377037\n",
      "38 epoch's loss: 0.0355038084089756\n",
      "39 epoch's loss: 0.033817339688539505\n",
      "the result of y when x is 4: tensor([7.6401], grad_fn=<AddBackward0>)\n",
      "model.parameter: [Parameter containing:\n",
      "tensor([[1.7916]], requires_grad=True), Parameter containing:\n",
      "tensor([0.4739], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "#print(torch.__version__)\n",
    "\n",
    "# train data \n",
    "x_data= torch.arange(1.0,4.0,1.0)\n",
    "x_data=x_data.view(-1,1)\n",
    "y_data= torch.arange(2.0,7.0,2.0)\n",
    "y_data= y_data.view(-1,1)\n",
    "\n",
    "# 超参数设置\n",
    "learning_rate=0.1\n",
    "num_epoches=40\n",
    "\n",
    "# 线性回归模型\n",
    "class LinearRegression(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(1,1)# 1 in and 1 out\n",
    "        \n",
    "    def forward(self,x):\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "# 定义loss function损失函数和optimizer优化器\n",
    "# PyTorch0.4以后，使用reduction参数控制损失函数的输出行为\n",
    "criterion = torch.nn.MSELoss(reduction='mean')\n",
    "# nn.Parameter - 张量的一种，当它作为一个属性分配给一个Module时，它会被自动注册为一个参数。\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
    "\n",
    "# 训练模型\n",
    "for epoch in range(num_epoches):\n",
    "    # forward \n",
    "    y_pred= model(x_data)\n",
    "    \n",
    "    #computing loss \n",
    "    loss = criterion(y_pred,y_data)\n",
    "    \n",
    "    print(epoch,'epoch\\'s loss:',loss.item())\n",
    "    \n",
    "    # backward: zero gradients + backward + step\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()  \n",
    "    optimizer.step() # 执行一步-梯度下降（1-step gradient descent）\n",
    "    \n",
    "# testing\n",
    "x_test=torch.Tensor([4.0])\n",
    "print(\"the result of y when x is 4:\",model(x_test))\n",
    "print('model.parameter:',list(model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6、pytorch实现一个简单的神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 37339276.0\n",
      "1 32173266.0\n",
      "2 31042600.0\n",
      "3 28464808.0\n",
      "4 22419230.0\n",
      "5 14968049.0\n",
      "6 8754934.0\n",
      "7 4915278.5\n",
      "8 2858272.5\n",
      "9 1815012.5\n",
      "10 1267675.75\n",
      "11 957740.75\n",
      "12 763809.0\n",
      "13 630301.75\n",
      "14 531367.6875\n",
      "15 454118.03125\n",
      "16 391833.3125\n",
      "17 340481.78125\n",
      "18 297555.9375\n",
      "19 261294.828125\n",
      "20 230395.375\n",
      "21 203855.46875\n",
      "22 180939.140625\n",
      "23 161059.265625\n",
      "24 143749.625\n",
      "25 128589.5078125\n",
      "26 115276.234375\n",
      "27 103567.65625\n",
      "28 93234.8125\n",
      "29 84100.59375\n",
      "30 76001.4375\n",
      "31 68812.7421875\n",
      "32 62404.8828125\n",
      "33 56675.515625\n",
      "34 51542.1640625\n",
      "35 46933.56640625\n",
      "36 42790.19140625\n",
      "37 39056.203125\n",
      "38 35695.57421875\n",
      "39 32657.349609375\n",
      "40 29905.755859375\n",
      "41 27411.2421875\n",
      "42 25147.029296875\n",
      "43 23089.986328125\n",
      "44 21218.01171875\n",
      "45 19513.357421875\n",
      "46 17959.5234375\n",
      "47 16541.388671875\n",
      "48 15245.890625\n",
      "49 14061.349609375\n",
      "50 12978.9248046875\n",
      "51 11987.8173828125\n",
      "52 11080.0849609375\n",
      "53 10247.0869140625\n",
      "54 9482.283203125\n",
      "55 8779.4462890625\n",
      "56 8133.14697265625\n",
      "57 7538.25830078125\n",
      "58 6990.724609375\n",
      "59 6486.1494140625\n",
      "60 6021.005859375\n",
      "61 5591.80615234375\n",
      "62 5195.609375\n",
      "63 4830.39453125\n",
      "64 4493.55859375\n",
      "65 4181.95849609375\n",
      "66 3893.6181640625\n",
      "67 3626.593017578125\n",
      "68 3379.22412109375\n",
      "69 3149.9296875\n",
      "70 2937.4091796875\n",
      "71 2740.203125\n",
      "72 2557.096435546875\n",
      "73 2387.0234375\n",
      "74 2228.998779296875\n",
      "75 2082.080810546875\n",
      "76 1945.495361328125\n",
      "77 1818.4130859375\n",
      "78 1700.1895751953125\n",
      "79 1590.13623046875\n",
      "80 1487.6322021484375\n",
      "81 1392.1536865234375\n",
      "82 1303.124267578125\n",
      "83 1220.1279296875\n",
      "84 1142.6990966796875\n",
      "85 1070.4552001953125\n",
      "86 1003.0167846679688\n",
      "87 940.0635986328125\n",
      "88 881.3096313476562\n",
      "89 826.5149536132812\n",
      "90 775.302490234375\n",
      "91 727.4146728515625\n",
      "92 682.6192626953125\n",
      "93 640.7335815429688\n",
      "94 601.5408935546875\n",
      "95 564.88232421875\n",
      "96 530.5509033203125\n",
      "97 498.40509033203125\n",
      "98 468.31622314453125\n",
      "99 440.1184997558594\n",
      "100 413.6933898925781\n",
      "101 388.91748046875\n",
      "102 365.6914978027344\n",
      "103 343.9131164550781\n",
      "104 323.48394775390625\n",
      "105 304.32061767578125\n",
      "106 286.34124755859375\n",
      "107 269.47100830078125\n",
      "108 253.63070678710938\n",
      "109 238.75177001953125\n",
      "110 224.7878875732422\n",
      "111 211.66558837890625\n",
      "112 199.34262084960938\n",
      "113 187.75924682617188\n",
      "114 176.87637329101562\n",
      "115 166.64393615722656\n",
      "116 157.02728271484375\n",
      "117 147.9823760986328\n",
      "118 139.47689819335938\n",
      "119 131.47781372070312\n",
      "120 123.95330810546875\n",
      "121 116.87138366699219\n",
      "122 110.2103042602539\n",
      "123 103.93927001953125\n",
      "124 98.03614044189453\n",
      "125 92.47942352294922\n",
      "126 87.2471694946289\n",
      "127 82.32170104980469\n",
      "128 77.68051147460938\n",
      "129 73.31040954589844\n",
      "130 69.19278717041016\n",
      "131 65.31398010253906\n",
      "132 61.65668487548828\n",
      "133 58.21287155151367\n",
      "134 54.964439392089844\n",
      "135 51.902626037597656\n",
      "136 49.015926361083984\n",
      "137 46.29780960083008\n",
      "138 43.73255920410156\n",
      "139 41.31387710571289\n",
      "140 39.031944274902344\n",
      "141 36.87943649291992\n",
      "142 34.848411560058594\n",
      "143 32.93321990966797\n",
      "144 31.125282287597656\n",
      "145 29.418682098388672\n",
      "146 27.808429718017578\n",
      "147 26.28839874267578\n",
      "148 24.853309631347656\n",
      "149 23.49808692932129\n",
      "150 22.218847274780273\n",
      "151 21.010986328125\n",
      "152 19.87051773071289\n",
      "153 18.793109893798828\n",
      "154 17.7750301361084\n",
      "155 16.813785552978516\n",
      "156 15.906188011169434\n",
      "157 15.048345565795898\n",
      "158 14.238126754760742\n",
      "159 13.472163200378418\n",
      "160 12.748077392578125\n",
      "161 12.063888549804688\n",
      "162 11.417634963989258\n",
      "163 10.806108474731445\n",
      "164 10.228435516357422\n",
      "165 9.682077407836914\n",
      "166 9.165329933166504\n",
      "167 8.676941871643066\n",
      "168 8.214874267578125\n",
      "169 7.778013229370117\n",
      "170 7.364890098571777\n",
      "171 6.973989963531494\n",
      "172 6.604685306549072\n",
      "173 6.254981517791748\n",
      "174 5.923886299133301\n",
      "175 5.611052513122559\n",
      "176 5.3150248527526855\n",
      "177 5.034661293029785\n",
      "178 4.769091606140137\n",
      "179 4.518221855163574\n",
      "180 4.280740737915039\n",
      "181 4.056221008300781\n",
      "182 3.843313694000244\n",
      "183 3.6418066024780273\n",
      "184 3.4509267807006836\n",
      "185 3.2706003189086914\n",
      "186 3.099869966506958\n",
      "187 2.938004970550537\n",
      "188 2.784754753112793\n",
      "189 2.6397275924682617\n",
      "190 2.502173662185669\n",
      "191 2.3720107078552246\n",
      "192 2.2488653659820557\n",
      "193 2.132066249847412\n",
      "194 2.0214383602142334\n",
      "195 1.9166496992111206\n",
      "196 1.8173985481262207\n",
      "197 1.7233340740203857\n",
      "198 1.6343145370483398\n",
      "199 1.5498794317245483\n",
      "200 1.4698162078857422\n",
      "201 1.3939621448516846\n",
      "202 1.3222076892852783\n",
      "203 1.254095196723938\n",
      "204 1.1896333694458008\n",
      "205 1.1285059452056885\n",
      "206 1.070510745048523\n",
      "207 1.01557195186615\n",
      "208 0.9635322690010071\n",
      "209 0.914168119430542\n",
      "210 0.8672951459884644\n",
      "211 0.8228703737258911\n",
      "212 0.780880331993103\n",
      "213 0.740964412689209\n",
      "214 0.7032099962234497\n",
      "215 0.6672660112380981\n",
      "216 0.6332924962043762\n",
      "217 0.6010215282440186\n",
      "218 0.5704261660575867\n",
      "219 0.5414431095123291\n",
      "220 0.5138458013534546\n",
      "221 0.4877924919128418\n",
      "222 0.4630308449268341\n",
      "223 0.4395679235458374\n",
      "224 0.41724514961242676\n",
      "225 0.39611780643463135\n",
      "226 0.376072496175766\n",
      "227 0.3570452928543091\n",
      "228 0.338971346616745\n",
      "229 0.32186877727508545\n",
      "230 0.30565497279167175\n",
      "231 0.290217787027359\n",
      "232 0.2755841016769409\n",
      "233 0.2616846561431885\n",
      "234 0.2485102117061615\n",
      "235 0.23607411980628967\n",
      "236 0.22420066595077515\n",
      "237 0.2129339873790741\n",
      "238 0.2022383213043213\n",
      "239 0.19207555055618286\n",
      "240 0.18244799971580505\n",
      "241 0.17331047356128693\n",
      "242 0.1645985245704651\n",
      "243 0.15636925399303436\n",
      "244 0.148514062166214\n",
      "245 0.14115551114082336\n",
      "246 0.13409152626991272\n",
      "247 0.1274036020040512\n",
      "248 0.12105156481266022\n",
      "249 0.11497943848371506\n",
      "250 0.10923559218645096\n",
      "251 0.10381604731082916\n",
      "252 0.09865789115428925\n",
      "253 0.09376101195812225\n",
      "254 0.08908043801784515\n",
      "255 0.08466469496488571\n",
      "256 0.08045938611030579\n",
      "257 0.07647036761045456\n",
      "258 0.07269106060266495\n",
      "259 0.06908198446035385\n",
      "260 0.06566116213798523\n",
      "261 0.06240340322256088\n",
      "262 0.05931273102760315\n",
      "263 0.05636277794837952\n",
      "264 0.053580157458782196\n",
      "265 0.05094458535313606\n",
      "266 0.04842124879360199\n",
      "267 0.046034075319767\n",
      "268 0.043758004903793335\n",
      "269 0.04161642864346504\n",
      "270 0.039564453065395355\n",
      "271 0.037616826593875885\n",
      "272 0.03577147051692009\n",
      "273 0.03401694446802139\n",
      "274 0.032345961779356\n",
      "275 0.03077828139066696\n",
      "276 0.029264872893691063\n",
      "277 0.02782800793647766\n",
      "278 0.02647322602570057\n",
      "279 0.025178272277116776\n",
      "280 0.02395094558596611\n",
      "281 0.022791769355535507\n",
      "282 0.02166803926229477\n",
      "283 0.02062598243355751\n",
      "284 0.019614482298493385\n",
      "285 0.018671292811632156\n",
      "286 0.017760252580046654\n",
      "287 0.01689479872584343\n",
      "288 0.016080442816019058\n",
      "289 0.015302522107958794\n",
      "290 0.014572102576494217\n",
      "291 0.013864429667592049\n",
      "292 0.013199240900576115\n",
      "293 0.012567296624183655\n",
      "294 0.011970054358243942\n",
      "295 0.011389907449483871\n",
      "296 0.010850854218006134\n",
      "297 0.010328846983611584\n",
      "298 0.00983500201255083\n",
      "299 0.009372237138450146\n",
      "300 0.008930183947086334\n",
      "301 0.008510079234838486\n",
      "302 0.008108751848340034\n",
      "303 0.007728034630417824\n",
      "304 0.007374496199190617\n",
      "305 0.007027846295386553\n",
      "306 0.006701731123030186\n",
      "307 0.006388712674379349\n",
      "308 0.006095605436712503\n",
      "309 0.005813096184283495\n",
      "310 0.0055452934466302395\n",
      "311 0.00529508525505662\n",
      "312 0.0050541674718260765\n",
      "313 0.004818946123123169\n",
      "314 0.004599119536578655\n",
      "315 0.004394590388983488\n",
      "316 0.004197564907371998\n",
      "317 0.004007856827229261\n",
      "318 0.0038336871657520533\n",
      "319 0.003666382050141692\n",
      "320 0.003507041372358799\n",
      "321 0.003356117056682706\n",
      "322 0.003207008121535182\n",
      "323 0.003068248275667429\n",
      "324 0.0029363716021180153\n",
      "325 0.0028105261735618114\n",
      "326 0.0026920537929981947\n",
      "327 0.002580262254923582\n",
      "328 0.0024702870287001133\n",
      "329 0.0023662401363253593\n",
      "330 0.002268360462039709\n",
      "331 0.002172408625483513\n",
      "332 0.002082783030346036\n",
      "333 0.001995694823563099\n",
      "334 0.0019148336723446846\n",
      "335 0.0018357320223003626\n",
      "336 0.0017645475454628468\n",
      "337 0.0016962633235380054\n",
      "338 0.0016289262566715479\n",
      "339 0.0015647929394617677\n",
      "340 0.0015048932982608676\n",
      "341 0.0014452524483203888\n",
      "342 0.0013891709968447685\n",
      "343 0.0013364312471821904\n",
      "344 0.0012859833659604192\n",
      "345 0.001236992422491312\n",
      "346 0.0011912981281057\n",
      "347 0.0011480548419058323\n",
      "348 0.0011043627746403217\n",
      "349 0.0010649553732946515\n",
      "350 0.0010261167772114277\n",
      "351 0.0009897742420434952\n",
      "352 0.0009517126018181443\n",
      "353 0.0009184236405417323\n",
      "354 0.0008876725332811475\n",
      "355 0.0008568603661842644\n",
      "356 0.0008272015256807208\n",
      "357 0.0007980494992807508\n",
      "358 0.0007718472043052316\n",
      "359 0.0007458736072294414\n",
      "360 0.0007211831980384886\n",
      "361 0.0006959904567338526\n",
      "362 0.0006740345270372927\n",
      "363 0.0006509483209811151\n",
      "364 0.0006293908227235079\n",
      "365 0.0006105132051743567\n",
      "366 0.0005919690011069179\n",
      "367 0.0005733462166972458\n",
      "368 0.0005555491079576313\n",
      "369 0.0005384053802117705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370 0.0005208004731684923\n",
      "371 0.0005041260737925768\n",
      "372 0.0004887660034000874\n",
      "373 0.0004745431651826948\n",
      "374 0.000460501410998404\n",
      "375 0.0004465484234970063\n",
      "376 0.00043414757237769663\n",
      "377 0.0004217565874569118\n",
      "378 0.0004090385336894542\n",
      "379 0.00039788481080904603\n",
      "380 0.0003865223261527717\n",
      "381 0.00037504968349821866\n",
      "382 0.00036443924182094634\n",
      "383 0.0003542876511346549\n",
      "384 0.0003446405753493309\n",
      "385 0.00033505988540127873\n",
      "386 0.00032630187342874706\n",
      "387 0.00031689958996139467\n",
      "388 0.0003085143107455224\n",
      "389 0.0003011942026205361\n",
      "390 0.0002924320288002491\n",
      "391 0.0002846772549673915\n",
      "392 0.0002779839560389519\n",
      "393 0.0002705492661334574\n",
      "394 0.00026358410832472146\n",
      "395 0.00025685917353257537\n",
      "396 0.00025026145158335567\n",
      "397 0.00024394251522608101\n",
      "398 0.00023757883172947913\n",
      "399 0.00023199839051812887\n",
      "400 0.00022628196165896952\n",
      "401 0.0002209479862358421\n",
      "402 0.0002156805421691388\n",
      "403 0.0002102191501762718\n",
      "404 0.00020559021504595876\n",
      "405 0.00020071497419849038\n",
      "406 0.0001956498745130375\n",
      "407 0.0001911672152346\n",
      "408 0.00018737545178737491\n",
      "409 0.00018294394249096513\n",
      "410 0.00017947162268683314\n",
      "411 0.00017480793758295476\n",
      "412 0.00017108921019826084\n",
      "413 0.0001679798006080091\n",
      "414 0.0001636685337871313\n",
      "415 0.00015995197463780642\n",
      "416 0.0001567052968312055\n",
      "417 0.0001530041336081922\n",
      "418 0.00015010016795713454\n",
      "419 0.00014698134327773005\n",
      "420 0.00014355983876157552\n",
      "421 0.00014103527064435184\n",
      "422 0.00013807869981974363\n",
      "423 0.0001350599341094494\n",
      "424 0.000132438974105753\n",
      "425 0.0001295742840738967\n",
      "426 0.0001269891217816621\n",
      "427 0.00012432590301614255\n",
      "428 0.00012174286530353129\n",
      "429 0.00011950951011385769\n",
      "430 0.00011717592860804871\n",
      "431 0.0001150341413449496\n",
      "432 0.0001126981369452551\n",
      "433 0.00011118816473754123\n",
      "434 0.00010887259850278497\n",
      "435 0.00010672087228158489\n",
      "436 0.00010446117812534794\n",
      "437 0.00010235942318104208\n",
      "438 0.00010100502549903467\n",
      "439 9.91432461887598e-05\n",
      "440 9.727317228680477e-05\n",
      "441 9.596438030712306e-05\n",
      "442 9.419739944860339e-05\n",
      "443 9.247146954294294e-05\n",
      "444 9.082870383281261e-05\n",
      "445 8.933193021221086e-05\n",
      "446 8.789778803475201e-05\n",
      "447 8.626603812444955e-05\n",
      "448 8.515890658600256e-05\n",
      "449 8.34656020742841e-05\n",
      "450 8.247813093475997e-05\n",
      "451 8.100784179987386e-05\n",
      "452 7.935730536701158e-05\n",
      "453 7.797990110702813e-05\n",
      "454 7.661353447474539e-05\n",
      "455 7.560285303043202e-05\n",
      "456 7.455289596691728e-05\n",
      "457 7.348622602876276e-05\n",
      "458 7.216045923996717e-05\n",
      "459 7.077975897118449e-05\n",
      "460 6.969217793084681e-05\n",
      "461 6.887386552989483e-05\n",
      "462 6.778905662940815e-05\n",
      "463 6.70134904794395e-05\n",
      "464 6.591605779249221e-05\n",
      "465 6.494684930657968e-05\n",
      "466 6.398817640729249e-05\n",
      "467 6.302592373685911e-05\n",
      "468 6.202217628015205e-05\n",
      "469 6.111432594479993e-05\n",
      "470 5.996308755129576e-05\n",
      "471 5.894009518669918e-05\n",
      "472 5.827736822539009e-05\n",
      "473 5.7411565649090335e-05\n",
      "474 5.650766979670152e-05\n",
      "475 5.597334893536754e-05\n",
      "476 5.531271017389372e-05\n",
      "477 5.449236778076738e-05\n",
      "478 5.394486652221531e-05\n",
      "479 5.304696242092177e-05\n",
      "480 5.22018781339284e-05\n",
      "481 5.1764447562163696e-05\n",
      "482 5.1107886974932626e-05\n",
      "483 5.03917399328202e-05\n",
      "484 4.9601290811551735e-05\n",
      "485 4.898475162917748e-05\n",
      "486 4.835459549212828e-05\n",
      "487 4.755411282530986e-05\n",
      "488 4.7004559746710584e-05\n",
      "489 4.6353201469173655e-05\n",
      "490 4.603260094881989e-05\n",
      "491 4.538303619483486e-05\n",
      "492 4.4710694055538625e-05\n",
      "493 4.403391358209774e-05\n",
      "494 4.3687181459972635e-05\n",
      "495 4.3007843487430364e-05\n",
      "496 4.264782910468057e-05\n",
      "497 4.205884033581242e-05\n",
      "498 4.145508137298748e-05\n",
      "499 4.095376061741263e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    " \n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    " \n",
    "# N batch_size; D_in 输入维度;\n",
    "# H隐藏层维度; D_out 输出维度.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    " \n",
    "# 生成数据.\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    " \n",
    "# 生成权重.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    " \n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.item())\n",
    "    #利用backward命令进行向后传播，loss.backward()将会计算所有与loss有关且requires_grad=True的变量的梯度\n",
    "    #在本例中将会计算loss对w1和w2的梯度，并可由w1.grad和w2.grad获取\n",
    "    loss.backward()\n",
    "    #w1和w2都是requires_grad=Ture,但是在更新w1和w2的时候我们并不希望计算这一步骤的梯度\n",
    "    #因此在更新w时，要用到torch.no_grad()\n",
    "    #这一步也可以通过torch.optim.SGD自动实现\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        \n",
    "        # 在更新完权重后，将梯度值进行重置\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在完成本次任务时，参考了如下：\n",
    "- https://www.bilibili.com/video/av49008640\n",
    "- https://github.com/xiaoming3526/Pytorch_learning/blob/master/Task2/README.md\n",
    "- https://github.com/ZJUTSong/PyTorch_Learning/blob/master/PyTorch_2.md"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
